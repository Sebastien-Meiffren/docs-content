---
meta:
  title: How to use Apache NiFi to migrate Object Storage data
  description: How to install and setup Apache NiFi to migrate data between clouds
content:
  h1: How to use Apache NiFi to migrate Object Storage data
  paragraph: How to install and setup Apache NiFi to migrate data between clouds
tags: Object-Storage Apache NiFi data-migration Terraform
categories:
  - storage
  - object-storage
dates:
  validation: 2022-02-17
  posted: 2019-09-26
---

## Apache NiFi - Overview

[Apache NiFi](https://nifi.apache.org/) is a system to process and distribute data between different machines. It can carry out managed data transfers between different source and destination systems. The tool provides a web interface to facilitate the design, management, and control of data transfers.

In this tutorial, we use Terraform to provision an Instance with appropriate [security group](/compute/instances/concepts/#security-group) settings, and with Apache Nifi installed. We then configure Apache NiFi to carry out the required data workflows.

<Message type="requirement">

- You have an account and are logged into the [Scaleway Console](https://console.scaleway.com)

</Message>

## Configuring Terraform

Install Terraform and export the variables that are necessary to run it:

1. Download the terraform binary from the official page [terraform.io](https://www.terraform.io/downloads.html). Once downloaded, copy it in your binary path.

2. Set the following variables to be able to provision an Instance with Terraform.
  ```
  export SCW_ACCESS_KEY="<ACCESS_KEY>"
  export SCW_SECRET_KEY="<SECRET_KEY>"
  export SCW_DEFAULT_ORGANIZATION_ID="<ORG_ID>"
  export SCALEWAY_REGION=fr-par
  ```
  <Message type="important">
  
  Replace `<ORG_ID>`, `<ACCESS_KEY>` and `<SECRET_KEY>` with the credentials of your [API key](/console/my-project/how-to/generate-api-key/).

  </Message>

## Running Terraform

1. Open a text editor and copy and paste the following code, editing it according to your needs. Save it as a Terraform plan, with the `.tf` extension. For example, call your file: `scaleway.tf`:

    <Message type="important">

    Replace `<MY_PUBLIC_IP>` with your public ip which can be found [here](https://whatismyipaddress.com), and `<APACHE_MIRROR>` by one of the URLs listed [here](https://www.apache.org/dyn/closer.lua?path=/nifi/1.15.3/nifi-1.15.3-bin.tar.gz).

    </Message>

  ```
  terraform {
    required_providers {
      scaleway = {
        source = "scaleway/scaleway"
      }
    }
    required_version = ">= 0.13"
  }

  provider "scaleway" {
    zone = "fr-par-1"
  }

  # create Instance IP
  resource scaleway_instance_ip "nifi" {
  }

  # create security group
  resource "scaleway_instance_security_group" "nifi" {
    name = "sg-nifi"

    inbound_default_policy  = "drop"
    outbound_default_policy = "accept"

    inbound_rule {
      action = "accept"
      port   = "22"
      ip_range = "<MY_PUBLIC_IP>/32"
    }

    inbound_rule {
      action = "accept"
      port   = "8080"
      ip_range = "<MY_PUBLIC_IP>/32"
    }

  }

  # create Instance, give it the previously-created IP and put it in the previously-created security group
  resource "scaleway_instance_server" "nifi" {
    name  = "nifi"
    type  = "DEV1-L"
    image = "ubuntu-focal"
    tags  = [ "nifi" ]
    ip_id = scaleway_instance_ip.nifi.id

    security_group_id =  scaleway_instance_security_group.nifi.id

    connection {
      host         = scaleway_instance_ip.nifi.address
      user         = "root"
    }

    # define commands to carry out on the Instance once it is created
    provisioner "remote-exec" {
      inline = [
        "apt-get update",
        "DEBIAN_FRONTEND=noninteractive apt-get install -yq python openjdk-11-jre-headless",
        "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64"
        "mkdir -p /opt/",
        "wget -O /opt/nifi.tar.gz <APACHE_MIRROR>",
        "tar xzf /opt/nifi.tar.gz -C /opt/",
        "/opt/nifi-1.15.3/bin/nifi.sh start",
      ]
    }

  }
  ```

2. Run the following commands:
  ```
  terraform init
  terraform apply
  ```

  Terraform initialises, and applies the configuration to create the Instance and install Apache Nifi on it.

## Configuring Apache NiFi

1. Find the public IP of the Instance created by Terraform, using one of the following methods:
  - Navigate to the [Instances](https://console.scaleway.com/instance/servers) page of the Scaleway console, find your `nifi` Instance and copy the IP address from the Overview page. 
  - Run the command `terraform show | grep address` from your local machine. 
  
2. Open your browser by typing the following URL: `http://<INSTANCE_PUBLIC_IP>:8080/nifi/` (Important: NiFi may need some time to start. If the page does not load, wait a few minutes before you retry). You will be greeted by NiFi's interface.
<Lightbox src="scaleway-empty.png" alt="" />

2. Once the interface displays proceed to the next step which is creating data flows using processors.

## Synchronizing one bucket to another

To be able to synchronize a bucket to another, we will have to create a workflow.

1. Start by dragging the `Processor` icon onto the workspace, it will prompt the following window.
 
  <Lightbox src="scaleway-s3_operations.png" alt="" />

2. We first want to list the content of our bucket, so start by creating a `ListS3` processor. Double click the processor or open the settings by clicking on the dented wheel in the `Operate` window and navigate to the Properties tab. We will start by creating an `AWS Credential Provider service`. This will permit to setup once our Credentials, click `Create new service...`
 
  <Lightbox src="scaleway-select_aws_creds_controler.png" alt="" />

3. It will prompt you with a new window in which you can define the name.
  <Lightbox src="scaleway-aws_creds_controler.png" alt="" />

4. Click the small arrow at the right of the line, it will open the configuration window. This will send you to the `NiFi Flow Configuration` window. You will be able to set your S3 `<ACCESS_KEY>` and S3 `<SECRET_KEY>` by clicking on the dented wheel at the right and entering them in the separate credential fields.

  <Message type="important">

   Replace `<ACCESS_KEY>` and `<SECRET_KEY>` with the credentials of your [API key](/console/my-project/how-to/generate-api-key/).

  </Message>

  <Lightbox src="scaleway-creds.png" alt="" />

5. Once the credentials entered, return to the `S3 List` `Processor` settings, you will have to set three settings.

    - The endpoint URL (has to be set in each `Processor`)
    <Lightbox src="scaleway-set_endpoint_url.png" alt="" />
    - The source or destination bucket name (has to be set in each `Processor`)
    <Lightbox src="scaleway-bucket_name.png" alt="Set Bucket name" />
    - The listing method (only in `S3 List` `Processor`)
    <Lightbox src="scaleway-list_v2.png" alt="" />

6. Once these three options are set, create a `FetchS3Object`, configure it the same way, then create a `PutS3Object` and set the destination bucket name.

7. Then connect all `Processors` using arrows and create loops for failures.

It should look the following way:

<Lightbox src="scaleway-global_view.png" alt="" />

Then enable each `Processor`, and if there are no errors, it should start synchronizing from source bucket to destination bucket.

## Combining several data sources

You can combine different fetch source to import or migrate your data to Scaleway Elements Object Storage.

<Lightbox src="scaleway-available_fetch.png" alt="" />

Here is an example of a multi fetch with one upload:

<Lightbox src="scaleway-combination.png" alt="" />
